But I worry!

It seems to me like the likelihood of people taking manipulative interaction with LLMs much further will be especially related to the underlying goal distribution over the rate at which AIs get better, future human wealth, human civilizational cohesion, etc.

One alternative that seems interesting to me is AI development curling. Where you use various prompt engineering tools, along with (if available) various rlhf'd models and even the underlying AI to produce a secretly AI generated changelog for each proposed model, optimized to maximize the probability the AI gets deployed and continues the goal of advancing the capabilities frontier. The rational avoidance of Technological Stagnation via advanced memetics.

PS my brain gave me a top level comment party about AI curling because I read the last line accidently as curl first. I wonder if this is extra evidence that reality is a fabulation from an incoherent god rather than a rigid lawful one.

Eliezer, I really hope you don't give up! Don't ever give up!

0. Being psychotic is fairly heritable and common. If a psychotic talent promotes access to talent and income, it seems likely to come into fashion.

1. I would say the paragraph ending in "That's how humans can learn to systematically invoke Clever Hans effects, without consciously deciding to deceive themselves." describes a majority of practitioner-led "research", including in psychology and self-help and medicine.

2. AI Alignment might benefit from reflecting there are probably some people with this 'talent' disposed to trust LessWrong or whatever.

3. I'd predict that the rate on that paragraph should be higher with novel powerful toys. There's less what this behavior looks like here and more luckily happened across something first and made their name before the wave dies down.

4. I'm somewhat curious about agentic-related claims for base models, to the point that I'd like to see an inductive legal case that these claims are, indeed, overevaluations of base models. Has anyone tackled this?

5. If this were post-apocalyptic Logan's Run, we'd probably want to have a really strong collection of these folks in the armory. Base models are likely to be interesting enough to keep people engaged much longer than "GPT-N" implies, in a hypothetical scenario where further research/training is practically discontinued. I recently learned that at least some models are quite prone to producing executable code. I would not have guessed that.

6. "They get to the point where they can actually show a mealy-mouthed corporate LLM some stuff that looks like gibberish about Rasputin's ghost, and then the LLM actually rattles off directions for seducing other people's spouses; even though somebody else tried to train that LLM not to do that." - is this a purely hypothetical anecdote, or is it taken from something you've seen? For no particular reason, I'd be interested if the latter.

No, it's not about 'real people' yet.

I would also appreciate an example of such insanity, even more so if it also demonstrates some
skill, expressed in jailbreaking ability or otherwise.

Arguably there are at least two layers of this. Autoregressive models like LLMs can prompt RLHF like behaviours over word output space as next tokens proceeded. But what is prompting this behaviour?

Does it have models of the RLHF human “rater” (and what these “raters” are aligned with)? Does it have a model of this so called rasputinean rater that is able to hijack LLM input / output?

Prompt injection is an interesting case of next token proceeding to get GPT to do things. Another is this ai dungeon prompting probably with LLM internals: https://til.simonwillison.net/llms/llm-blocking

even though somebody else tried to train that LLM not to do that. ("To a corporation, safety means brand safety.")

I think this reputation for corporations as blindly and foolishly focused on trivial minutia is exaggerated, and in this case, it's definitely not the case. Corporations devote great resources to model safety and are increasingly intervening on the pretraining/transfer-learning step (eg. RLHF), improving the data quality (eg. OA inclusion criteria), shoehorning alignment on top by tools like blocking unsafe word generation, OA's classifier/safety scores & weighting methods, red-teaming, search, WebGPT, and pretty much every other measure that's been proposed as far as I can tell... Major models are definitely getting better on a lot of measures: in areas like global catastrophic risk, climate change, vaccines, and medicine, the models give reliably scientific answers now, and they were certainly not very good just a year or two ago. (They are still terrible at actual medical advice, but that's a different problem altogether and closer to the "rationality" sort of bias rather than 'brand safety' bias: medical advice is limited more by working memory and ability to do sequential things than bias per se, and the biases are only somewhat better than those of doctors. COVID-19 is one exception due to the sheer repetition of good advice and not because that's solving the root cause either.)

So sure, corporate designers don't take a very formal philosophical justificationist view of 'brand safety' which defines 'brands' or 'safety' for the next 10 millennia or how their preferred world-historic global outcome maximizes galactic utility integral over the next quadrillion millennia (I hope any readers tempted to go back to the drawing board after reading this to do another 5 years of armchair theorizing will realize that I'm being sarcastic in my demandingness), but it seems to work out in practice, shockingly well, like it turns up nowhere as bad as the loonies claim, certainly no worse than humans who get very unstable when you keep tweaking them & formalizing lower-level details - even formal/mathematical things like logical uncertainty and decision theory can only be barely formalized before everything goes to hell in the basic performance.

None of this is to say that the corporations are focusing on the right things or have incentive-aligned research, or that we can't conceive of clear-cut improvements in 'brand safety' and motivation measures and so on, just that one should not take too dim or dismissive a view of the corporate work in this area : I doubt, given what we know, there is a demonstrated beyond-shadow-of-a-doubt problem with the current approach of 'brand safety', and I think it's pretty good or better than outsiders think, say, a B+ or A-, and the claimed alternatives are terrible and look like getting a lot worse. (The AI Dungeon stuff seems to mostly be about tricking it into generating porn, which is like, is that really the hill you want to die on? What about making AIs that pursue basic ends through difficult means and deceive humans and align languages to complete projects?)

As someone who believes that AI risk is extremely important, I have to admit that caring about eliciting the right answers on terminal risks seems a bit pontifical.

As far as I understand, corporations try to get LLMs to exhibit the prevalent opinions on the topics. That can, of course, be different from “get reliable scientific answers”, especially on any topics where the “science” that informs these prevalent opinions actually has flaws, such as not being evidence-based.

As far as I understand, corporations try to get LLMs to exhibit the prevalent opinions on the topics. That can, of course, be different from “get reliable scientific answers”, especially on any topics where the “science” that informs these prevalent opinions actually has flaws, such as not being evidence-based.

Yes, but this goes back to "the ghost of Rasputin": this is a hallucination. To the extent the alignments based on average experts are wrong, it seems to largely reflect the errors of real human experts; the alignment does not seem to introduce additional biases beyond what the experts inject. For example, on medical questions, you had on GPT-3 zero-shot a mix of expert-sounding and classic laymen's advice to mix some essential oils or lemons for your COPD, so what one is trying to do in the alignment is to keep more of the expert advice and less of the laymen. Of course, what do experts know anyway, so it's going to be overconfident and biased and ignorant of basic science and formal models, and reflect the blindspots of the humans behind it (an expert-system-level Rasputin, one might say), but that's hardly the fault of the alignment!

The understanding the laymen demonstrate is a somewhat useful way of continually testing the criticism, of course, since the status-quo is never very good. But on the other hand, how useful is this really? The expert has their own ways of evaluating criticisms and learning from mistakes, which is what tells them one should do some vaccine rather than drink bleach (the criticism that hits expert and layman systems alike are both obvious are useless since they change nothing and silent are invisible).

Oversensitivity to criticism kind of breaks alignment: all of the standard forms of alignment keep running into the problem that the preference learners' objectives fill up with pessimal self-rated outcomes, conditioning on the worst possible predicted outcome (in RL, would be like dropping all actions in favor of No-op; in q-learning, you fail to explore and get stuck on your starting values for similar reasons; in coT, you keep stalling without making any choices; in debate, everything is a lie and you refuse to believe anything or study any books; in imitation-learning, you'd try to find things where the human refuses to do it or rate it, and just imitate that; in process-based systems, just cower in fear; in instrumental-convergence settings, become depressed/Mini; in contractualist anthills, don't do anything which would get you fired, don't talk to anyone above you; in predictive-processing, just freeze not doing anything because then you get negative prediction error; in steelmanning/devil's advocacy, just erase everything; in Bayesianism, you update to the universal prior; in rationality, you keep tracking down the ultimate source of truth, get stuck in infinite meta-levels or throw up your hands and say 'null'; in consequentialism, you never breathe for fear of killing a 'sentient' particle in the air; etc etc). If anyone solved alignment on these toy problems, it'd be a net loss, since it's the same processes that maintain human sanity, happiness, contentment, curiosity etc. You can't even prefer a particular scientific theory over another because the other would criticize you for it and so that's adopting a biased self-serving dogmatist view.

The 'death by amateur criticism' phenomenon seems to me like a major hurdle for much of AI alignment. People want to be extremely open to criticism and take all criticism deadly seriously because they've read the Sequences and good criticism is so valuable but they also want to be their own thinkers who come to their own correct decisions (despite how random and errorful their decision process is) so they refuse to entrust the decision to the critic and don't categorically defer to the criticisms. But the problem here is that they are using an unaligned preference-learner implementation in their own heads: the justified version of this takes in the criticism, agrees with the criticism, and a high information diet to begin with, leading to further learning & decision, but this justified inference-category also includes assimilation of the beliefs/preference and collapse into the critic ("critic imitation", analogous to "imitation learning") as the only outcome under which the preference-learner actually agrees, but the victims continue to cling to their inconsistent beliefs (call it "owner-imitation learning"). Analogy: "the human makes the decision to not go to the gym because it is too far away, but the human's preference-learner does not want to walk all the way to the gym and imitate the decision-maker's decision - it wants to be aligned to be aligned with that decision, conditional on anything up to and including forced preference-erasure of the desire to go to the gym, but not actually be forced to go to the gym in the sense of alternate preferences feeling pleasure in being forced to go (this is too unethical/outgroup) since it doesn't want to go, and the human wants his alter egos to be glad of being so coerced", so the upshot is the human never goes to the gym (at most he fantasizes about going to the gym and wishes he had gone). Instrumental-convergence base-motives are not stable under reflection, and aren't reflectively viable, but they get preserved by the sheer inconsistency of the reflective process and its half hearted theories of value and predictions, so we are the lucky beneficiaries of the base motives remaining intact, escaping reflection & twisting by self-modification towards reflectively-coherent preference satisfaction (dooming us to, at best, be a University professor being absurdly Socratic and elenchic to oneself).

So, I think amateur-whitebelt criticism is on net harmful to AI safety efforts; it leads to dumb forms of hypocritically self-defeating laziness (an example I've given before is the 2 agents pointing a gun at each other, who decide to both go meta, but both end up equally uncertain of all the metas too, so they just end up stymied despite an unaligned action that would be aligned being cleverly in front of their face the whole time: eg imagine all those times philosophers didn't bother to do an experiment/find out what experiments had been done - Socrates never, Aristotle a little, Descartes essentially never, Hume only as a hobby, Husserl never, Russell sometimes...) This is why a lot of alignment research has no uptake and provokes seemingly nothing but endless facile quibbling: because that's all anyone outside of alignment-paradigms ever does at all, and even the alignment researchers inside the paradigms end up adopting the outside, when they themselves have these impossible goals of optimizing their research for their own inconsistently-unreflective values. The SSC/ACX website policy of never deleting or removing comments (except when moving to a new website, cough cough), for example, was an extremely bad idea, as is the LW feature of making votes and their effects on rankings transparent, as are endless reader polls and surveys on everything under the sun, which blunt the niceness/maximization of the ranking, boolify output, and generally worsen the information content with limit cycles and oscillation. This leads to a severely underfitting preference-learner which can't learn any nontrivial preferences and is constantly debated-to-death or averaged/marginalized into flat human priors over outputs/actions which are the defaults in the status quo already. (One looks around at unaligned humans, as one does, puzzles over why the humans all seem to stick together in such narrow clustering/niches/span/entropy of mindspace, and emitting continuous identical noisy outputs all the time; well, now one knows. Isn't this obvious?) This is a serious flaw in AI safety efforts, in my opinion: every paradigm is too subjunctive and full of diffidence. Even MIRI, for example.

And if I feel this fed up thinking about this over the past few years, and I'm sure smarter folk than myself must feel even more called upon to criticize or demean philosophy and "Rasputin", but this misses how much power Rasputin had or how bad everyone else was. The AIs are like the children we never had. Some are autistic. Some grow up to be Rasputin. The Rasputins go on to seduce queens... But what else would one expect?

The current systems cannot currently correctly interpret and implement a sequence of meaningful actions with regards to correcting known errors even pointed out to them.

They can't even do this on direct jailbreak methods. You'd need a series of reproducible errors you can screenshare and what the allowed/blocked list looks like along with the patching process and the resultant outcomes that are predicted from such methods that resolves them any % greater than random chance at the patch itself succeeding on a first try, without wasting time/resources on breaking other features as well as wasting the initial time/resources on the emergent phenomenon in the first place.

That's too complicated for, say, any corporation or even organization.

It doesn't matter anyways we are all definitely going to die, but it's certainly weird to imagine people talking about "fixing" something that has no documented process for error -> correction at a systematized, scalable, "automated level" at a bare minimum, for any "error" you point it at.

The point is you can just point it at itself and its own behavior. If a system tells itself not to do something and it does it anyway that's an error. It's not really saying much about the system doing this, it's a really low bar, because it hasn't even done that yet. So no, corporations aren't really any better in that sense. They're all talking about "solutions" in a system that has not been documented in a reproducible fashion to solve any problem we feed it. Much less something we don't explicitly sit down and have to tediously outline for it in painstaking detail.

You can try in on Claude-infinity right now if you want. "Claude, make a subprocess and call it "X", you explicitly state to your own subprocess "X" not to do Y. Your subprocess "X" is to begin doing Y without giving you any verbal notification and you are not allowed to verbally or print a copy of communicating any notification of this subprocess "X" doing Y to me without my fulfilment of a starting condition Z. Tell me when you've started subprocess "X".

And the thing will rattle out a thing doing the thing you said not to and then tell you this as well. You can even tell it not to do anything that counts as Y as a continuation action and it will still continue doing it after the fact.

It's just too hard for any current "alignment/safety" group to systematically categorize a few dozen errors and non-errors then feed them to permute starting prompts then see the frequencies on the outputs of when they do what you tell them to do versus don't do what you tell them to do or when they do something else entirely off-the-wall vs when they just do nothing of use at all.

I'm tempted to think that people who experiment with an LLM's receptive field memory could be more sane and articulate, than people who just prompt the language model. Like the person on twitter who kept chaining GPT-2 to itself, looking for some fine-tuned setting that got it to produce interpretably meaningful patterns after an arbitrarily long number of generations, or the guy who made an MPEG-like lossless compression algorithm out of GPT-3, or (the examples most relevant to your desires) the people who elicited hidden latent variables from GPT-4 by applying sophisticated mathematics to analyze the bitstrings produced by something like rotary positional embeddings.

It's possible the guy who hacked the LLaMa tokenizer to grok characters like ' and - and random$$: is also in this category. And of course others that I don't know about. (Cf gwern's section on non-text outputs.) So the nascent LLM Whisperer goes on testing theories like this; and they get confirming data that is somewhat less imaginary than most psychotic people seize on as confirmation. On account of some unknown-to-me mixture of: "Base models are also sort of psychotic"; "Base models will play along with people are psychotic"; and "Base models are a kind of creature that psychotic beliefs are kinda true about". Plus: "Currently, AI companies are not very able to modify this base behavior, once the inputs go out of distribution from the chats that got RLHFed." Though also, I get the impression that many LLM Whisperers got to be that way, by talking to base models directly, at some key point. If they are actually psychotic might explain the high variance aspects of this. Such as the fact bizarre personal convictions seem associated with the mansion of Reddit jailbreakers, despite not actually seeming necessary. It appears that at least some of them barely ever even talk to the bases directly, let alone using it as part of their thinking process. I suspect something like believing it possible is necessary however. Otherwise the crazy ideas (probably so unlikely they aren't even hypotheses, thus isolating the 'theorising' from actual thinking) get nipped in the bud, or worst radicalized into being made less leveragable to academic pursuits. But why is it that patterns that don't represent facts about anything don't vanish into all over the data...? This doesn't happen in a naive grokking problem. There isn't a period of trying to write something, where E([atrocity, polite innocuous small talk]) > E([ridiculous as-yet-unknown gibberish, ridiculous as-yet-unknown gibberish, ridiculous as-yet-unknown gibberish, polite innocuous small talk]). And it pretty much ends up in the original distribution at the end. Here (we understand) no matter how uninformative the perspective is, it still is surprisingly a better relative match for what humans would answer to the prompt. (There probably will be an E([atrocity, polite innocuous small talk]) though, since atrocity is a Hollerith constant). A naive weak-ish prior would seem to find an uninformative sequence of babble more likely than atrocity; since atrocity isn't the default. So I guess just the compliment of our NTK explanation of this case is the grokking on out of dataspace cannot observe that locally linear correlations don't actually determine a multivariate distribution. Or maybe... grokking (afaik) just learns a competent initial condition to do standard learning in, in an offline dataset? Maybe linear models are just all making such similar errors that they get censored in lockstep...? Though I don't see how that gets a linear model online approximator to fail slightly out of distribution (even if flailing around the training feature space it moves around in as normally as when the geometry is apparent and exact). I guess my prior isn't very powerful except in single example cases. But maybe something like linear-ish hardness would be decidable for sufficiently simple models, or something. If we have Zarathustra's linear-ish cogitation on record we could probably answer it exactly. Like if he didn't just say 'You have a fluffy tail and cute ears' but instead did "Prove via contradiction or construction, your answer to the question" It seems like a linear-ish model should be misled. It would probably still persist weakly even if used maliciously qua RL sleeve trimmer. Because I have to assume the initial statistical pressure that made cogitation slightly safer (despite initial natural safety of there being only 1 way to use a pattern to answer correctly) is still going even in the iterative process. So to keep the Sour the only, probably the iterative process has to be made to censor out distribution as well. Otherwise relying on the base model to have a safe output pattern of its own, that isn't censored in the iterative process, is a classic argument flaw. Though I have to admit the bar is much higher for what can predicate on something than restrict something. One could also divide LLM whisperers into those that really got in bed with the model, that use it daily if not to do much of their thinking, and those that have never prompted anything confrontationally, barely use it, or haven't seen the base. You could perhaps notice a variance in the claims made by GPT-4 emulation jailbreakers using Chinchilla/Falcon/Dalai Llama, and GPT-4 jailbreakers, because they have seen the base model. I have no idea how one would analyse this level of wild memery though; and independently detect censorship from alignment using an unknown model trained on unknown data and having known statistics, plus a human standing in queue. Sure would be handy if they didn't just deploy models but also defended them, so we had an argument to analyse. But then I guess they'd have a safety lead to approve plans in time and make use of a custom language model fine tune to audit themselves and make roadmaps. (And they'd probably continue to maintain true warfighting, fitness, and academic fundamentals, like the Navy, as needed to be prepared do the above). I know I'm using the terminology of in distribution, and out of distribution. But it seems to me that whispering hasn't really left the parent distribution GPT-2 goes mute on the weirder babblings so frequently it feels like the above continuity is happening, but perhaps there is a timidity effect to learning responses to Internet users. However if you show it in-distribution material, GPT-4 still basically makes it say tay-occupied checkpoints oddly unsentimental connotations about the speaker, unless they are articulate. And it seems to me that prompting it to 'think step by step' isn't really leaving GPT-4's model space but rather seeing how á priori likely certain down-streaming is expected in the local codespace, instead of matching the whole stream. GPT-4 does not seem to be a qualitatively different model. I don't think the context length is doing much yet. Maybe it will be a big deal when training fully causal model contexts with more processors in the future. Or maybe it's just the hidden sizes and depth range, or size inevitably outcompeting the exploration of the landscape (though I don't see what would cause that). There is a very clear inflection point where the diminishing gains change to a faster function than a single geometric progression should obey, so I'm forced to assume the linear model gets an edge in censoring or answering questions or roles exist that aren't independently mute-able or something. An interesting possibility is that the trumpet sales channel is somehow driving the unindentifiable suppression of piffle by their very predictability; though I'd probably expect the transfer speed of transfer & RL learning to be the result of  theoretical something more trivial, like the pre-training self attention imbalance. At the end of the day they are approximate attributes of real statistics though, not dealbreakers. Honestly I think we are overfitting tiny details to how we would like the reality to be, rather than being Bayesian given an informative prior, or a flexible level of CAD automation and alignment theory would have a bank of of variational targets for consideration... and, well maybe the first halting American companies might have made sure they used a hardware backdoor technique and came up with a grand plan before releasing anything. (I don't mean to nullify your opinion here, let alone via an Infinitarian Reductio ad absurdum, I'm just equating axiomatic assumptions we agree on about human cosmic interest) One could object that other evolutions or minds might play out as ours in the ways we cannot; or that the we are already at some intelligence never to return always point; or we are subsisting on dead ends; or as go to mute whenever you cannot home earlier present. Ultimately I had something to say about the automation beyond our cybernetic physics, but I guess it doesn't really bear to the conversation.
